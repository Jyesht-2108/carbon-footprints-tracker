# Carbon Nexus - Complete Workflow Explained

## ğŸ“Š Database Schema Overview

### Total Tables: 12 Tables across 3 Plugins

#### Data Core Plugin (4 tables)
1. `events_raw` - Raw uploaded data
2. `events_normalized` - Cleaned & normalized data
3. `data_quality` - Quality metrics per supplier
4. `ingest_jobs` - Upload job tracking

#### RAG Chatbot Plugin (3 tables)
5. `uploads` - PDF documents uploaded
6. `ingestion_jobs` - PDF processing status
7. `recommendations` - AI-generated recommendations

#### Orchestration Engine (5 tables)
8. `hotspots` - Detected emission anomalies
9. `alerts` - Generated alerts
10. `baselines` - Entity baseline emissions
11. `predictions` - ML prediction cache
12. `audit_logs` - Action audit trail

---

## ğŸ”„ Complete Data Flow

### Phase 1: Data Upload & Normalization

```
USER UPLOADS CSV
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA CORE (Port 8002)                   â”‚
â”‚ POST /api/v1/ingest/csv                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
1. Create job in `ingest_jobs` table
   - job_id: "uuid-123"
   - status: "processing"
   - filename: "emissions.csv"
    â†“
2. Insert raw data into `events_raw` table
   - id, supplier_id, timestamp, payload (JSONB)
    â†“
3. Validate & Normalize
   - Check required columns
   - Convert units (km, kg)
   - Standardize vehicle types
   - Detect outliers (IQR method)
   - Fill missing values (ML-based)
    â†“
4. Insert into `events_normalized` table
   - id, event_type, supplier_id
   - distance_km, load_kg
   - vehicle_type, fuel_type
   - speed, timestamp
   - is_outlier (boolean)
    â†“
5. Calculate quality metrics
   Insert into `data_quality` table
   - supplier_id
   - completeness_pct
   - predicted_pct
   - anomalies_count
    â†“
6. Update job status
   `ingest_jobs.status = "completed"`
```

**Result:** Clean, normalized data ready for ML predictions

---

### Phase 2: ML Prediction (Automatic - Every 5 min)

```
ORCHESTRATION SCHEDULER RUNS
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ORCHESTRATION ENGINE (Port 8003)        â”‚
â”‚ Scheduled Task: Hotspot Scan            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
1. Fetch events from `events_normalized`
   SELECT * FROM events_normalized
   WHERE id NOT IN (SELECT event_id FROM predictions)
   LIMIT 50
    â†“
2. For each event, call ML ENGINE
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ML ENGINE (Port 8001)                   â”‚
â”‚ POST /api/v1/predict/logistics          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Request:
   {
     "distance_km": 120,
     "load_kg": 450,
     "vehicle_type": "truck_diesel",
     "fuel_type": "diesel",
     "avg_speed": 50
   }
   
   Response:
   {
     "co2_kg": 18.1,
     "model_version": "v1.0",
     "confidence": 0.92
   }
    â†“
3. Cache prediction in `predictions` table
   - event_id: 22
   - prediction_type: "logistics"
   - predicted_co2: 18.1
   - confidence_score: 0.92
   - model_version: "v1.0"
   - features: {...} (JSONB)
```

**Result:** ML predictions cached for all events

---

### Phase 3: Hotspot Detection

```
ORCHESTRATION ENGINE
    â†“
1. Get baseline for entity
   SELECT baseline_value FROM baselines
   WHERE entity = 'Supplier A'
   AND entity_type = 'supplier'
   
   If no baseline exists:
   - Calculate from historical average
   - INSERT INTO baselines
    â†“
2. Compare prediction vs baseline
   predicted_co2 = 120 kg
   baseline_co2 = 60 kg
   percent_above = (120 - 60) / 60 * 100 = 100%
    â†“
3. Determine severity
   IF percent_above >= 150%: severity = "critical"
   ELSE IF percent_above >= 100%: severity = "warn"
   ELSE IF percent_above >= 80%: severity = "info"
   ELSE: No hotspot
    â†“
4. Insert into `hotspots` table
   {
     "entity": "Supplier A",
     "entity_type": "supplier",
     "predicted_co2": 120,
     "baseline_co2": 60,
     "percent_above": 100,
     "severity": "critical",
     "status": "active",
     "event_id": 22
   }
```

**Result:** Hotspot detected and stored

---

### Phase 4: Alert Generation

```
ORCHESTRATION ENGINE
    â†“
When hotspot is created:
    â†“
1. Generate alert message
   message = "Supplier A exceeded emissions by 100%"
    â†“
2. Insert into `alerts` table
   {
     "level": "critical",
     "message": "Supplier A exceeded emissions by 100%",
     "hotspot_id": 1,
     "acknowledged": false
   }
    â†“
3. Push to WebSocket (future)
   ws.emit('alert', alert_data)
```

**Result:** Alert created and ready for frontend

---

### Phase 5: AI Recommendation Generation

```
ORCHESTRATION ENGINE
    â†“
When hotspot is detected:
    â†“
1. Call RAG SERVICE
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG CHATBOT (Port 4000)                 â”‚
â”‚ POST /api/rag/recommend                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Request:
   {
     "supplier": "Supplier A",
     "predicted": 120,
     "baseline": 60,
     "hotspot_reason": "Emissions 100% above baseline",
     "hotspot_id": 1
   }
    â†“
2. RAG uses Gemini AI to analyze
   - Reads context from uploaded PDFs (Qdrant)
   - Generates structured recommendations
    â†“
   Response:
   {
     "root_cause": "Increased load due to holiday demand",
     "actions": [
       {
         "title": "Shift 20% load to Supplier B",
         "description": "Redistribute load to reduce emissions",
         "co2_reduction": 22.5,
         "cost_impact": "+3%",
         "feasibility": 9,
         "confidence": 0.87
       }
     ],
     "saved": 1
   }
    â†“
3. RAG inserts into `recommendations` table
   {
     "hotspot_id": 1,
     "supplier_id": "Supplier A",
     "title": "Shift 20% load to Supplier B",
     "description": "Redistribute load...",
     "co2_reduction": 22.5,
     "cost_impact": "+3%",
     "feasibility": 9,
     "confidence": 0.87,
     "root_cause": "Increased load...",
     "status": "pending"
   }
```

**Result:** AI-powered recommendations stored

---

### Phase 6: Frontend Display

```
FRONTEND (Port 3000)
    â†“
1. Dashboard loads
   â†“
2. Fetch current emissions
   GET http://localhost:8003/emissions/current
   
   Response:
   {
     "current_rate": 120,
     "trend": "increasing",
     "last_updated": "2025-11-28T20:30:00Z"
   }
    â†“
3. Fetch hotspots
   GET http://localhost:8003/hotspots
   
   Response: [
     {
       "id": 1,
       "entity": "Supplier A",
       "predicted": 120,
       "baseline": 60,
       "percent_above": 100,
       "severity": "critical"
     }
   ]
    â†“
4. Fetch recommendations
   GET http://localhost:8003/recommendations
   
   Response: [
     {
       "id": 1,
       "title": "Shift 20% load to Supplier B",
       "co2_reduction": 22.5,
       "feasibility": 9,
       "status": "pending"
     }
   ]
    â†“
5. Fetch alerts
   GET http://localhost:8003/alerts
   
   Response: [
     {
       "id": 1,
       "level": "critical",
       "message": "Supplier A exceeded emissions by 100%",
       "acknowledged": false
     }
   ]
    â†“
6. Display on dashboard
   - Emission pulse chart
   - Hotspot heatmap
   - Recommendation cards
   - Alert notifications
```

**Result:** User sees complete intelligence dashboard

---

### Phase 7: User Actions

```
USER APPROVES RECOMMENDATION
    â†“
FRONTEND
POST http://localhost:8003/recommendations/1/approve
{
  "notes": "Approved by manager"
}
    â†“
ORCHESTRATION ENGINE
    â†“
1. Update `recommendations` table
   UPDATE recommendations
   SET status = 'approved'
   WHERE id = 1
    â†“
2. Insert audit log
   INSERT INTO audit_logs
   {
     "action": "approve_recommendation",
     "entity_type": "recommendation",
     "entity_id": 1,
     "notes": "Approved by manager"
   }
    â†“
3. Return success
   {
     "status": "approved",
     "recommendation_id": 1
   }
```

**Result:** Action tracked and audited

---

## ğŸ“Š Database Relationships

```
events_normalized (Data Core)
    â†“ (event_id)
predictions (Orchestration)
    â†“ (triggers)
hotspots (Orchestration)
    â†“ (hotspot_id)
    â”œâ”€â†’ alerts (Orchestration)
    â””â”€â†’ recommendations (RAG)

baselines (Orchestration)
    â†‘ (used by)
hotspots (for comparison)

audit_logs (Orchestration)
    â† (tracks all actions)
```

---

## ğŸ”„ Automatic Processes

### Every 5 Minutes (Scheduler)
1. Fetch new events from `events_normalized`
2. Call ML Engine for predictions
3. Cache in `predictions` table
4. Compare vs baselines
5. Detect hotspots â†’ Insert into `hotspots`
6. Generate alerts â†’ Insert into `alerts`
7. Call RAG for recommendations â†’ Insert into `recommendations`

### Every 1 Hour (Scheduler)
1. Recalculate baselines
2. Update `baselines` table with new averages

---

## ğŸ¯ What Creates What

### Data Core Creates:
- âœ… `events_raw` - Raw data
- âœ… `events_normalized` - Clean data
- âœ… `data_quality` - Quality metrics
- âœ… `ingest_jobs` - Job tracking

### ML Engine Creates:
- âœ… Predictions (returned via API, cached by Orchestration)

### Orchestration Engine Creates:
- âœ… `predictions` - ML prediction cache
- âœ… `hotspots` - Detected anomalies
- âœ… `alerts` - Alert notifications
- âœ… `baselines` - Entity baselines
- âœ… `audit_logs` - Action tracking

### RAG Chatbot Creates:
- âœ… `uploads` - PDF documents
- âœ… `ingestion_jobs` - PDF processing
- âœ… `recommendations` - AI recommendations

### Frontend Creates:
- âŒ Nothing directly
- âœ… Triggers actions via API (approve/reject)

---

## ğŸ”Œ Frontend Integration

### Frontend Expects (from .env):
```
VITE_API_URL=http://localhost:8000  â† WRONG! Should be 8003
VITE_WS_URL=ws://localhost:8000/ws  â† WRONG! Should be 8003
```

### âš ï¸ Frontend Configuration Issue

The frontend is configured for port **8000** but orchestration runs on **8003**.

**Fix needed:**
```env
VITE_API_URL=http://localhost:8003
VITE_WS_URL=ws://localhost:8003/ws
```

### Frontend API Calls:
- `GET /emissions/current` âœ… Available
- `GET /emissions/forecast` âœ… Available
- `GET /hotspots` âœ… Available
- `GET /recommendations` âœ… Available
- `GET /alerts` âœ… Available
- `POST /recommendations/{id}/approve` âœ… Available
- `POST /simulate` âœ… Available

All endpoints match! Just need to fix the port.

---

## ğŸ“ˆ Data Flow Summary

```
1. CSV Upload â†’ events_raw â†’ events_normalized
2. Scheduler â†’ ML Prediction â†’ predictions
3. Prediction + Baseline â†’ Hotspot Detection â†’ hotspots
4. Hotspot â†’ Alert Generation â†’ alerts
5. Hotspot â†’ RAG AI â†’ recommendations
6. Frontend â†’ API â†’ Display All Data
7. User Action â†’ API â†’ Update Status â†’ audit_logs
```

---

## ğŸ¯ Current Status

### âœ… Working:
- Data upload & normalization
- ML predictions (with correct API path)
- Database tables created
- Scheduler running
- All services connected

### â³ Pending:
- Hotspot detection (waiting for next scan)
- Alert generation (after hotspots)
- Recommendations (after hotspots)
- Frontend port configuration

### ğŸ”§ Action Needed:
1. Wait for next scheduler run (5 min) OR
2. Manually trigger: `curl -X POST http://localhost:8003/hotspots/scan`
3. Fix frontend .env port to 8003

---

## ğŸš€ Complete Flow Example

```
1. User uploads emissions.csv
   â†’ Data Core normalizes 22 events
   â†’ Stored in events_normalized

2. Scheduler runs (5 min later)
   â†’ Orchestration fetches 22 events
   â†’ Calls ML Engine 22 times
   â†’ Gets predictions (e.g., 18.1 kg CO2)
   â†’ Caches in predictions table

3. Hotspot detection
   â†’ Compares 18.1 vs baseline 60
   â†’ No hotspot (below 80% threshold)
   
   OR if prediction was 120:
   â†’ 120 vs 60 = 100% above
   â†’ Creates hotspot (severity: critical)
   â†’ Stored in hotspots table

4. Alert generation
   â†’ Creates alert for hotspot
   â†’ Stored in alerts table

5. RAG recommendation
   â†’ Calls Gemini AI
   â†’ Generates 3 recommendations
   â†’ Stored in recommendations table

6. Frontend displays
   â†’ Hotspot on heatmap
   â†’ Alert notification
   â†’ Recommendation cards
   â†’ User can approve/reject
```

---

## ğŸ“š Summary

**12 Tables** store all data across the system:
- **4 tables** for data ingestion (Data Core)
- **3 tables** for RAG & recommendations (RAG Chatbot)
- **5 tables** for orchestration & intelligence (Orchestration Engine)

**Everything is automatic** once data is uploaded:
- Scheduler runs every 5 minutes
- Detects hotspots
- Generates alerts
- Creates recommendations
- Frontend displays everything

**You just need to:**
1. Upload CSV data
2. Wait for scheduler OR trigger manually
3. Check frontend (after fixing port to 8003)

ğŸ‰ **The system is fully connected and working!**
